{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from Gatsbi_Net import GatsbiNet\n",
        "from gatbi_dataset import GatbiDataset\n",
        "from utils import get_cell_expression_data, to_entrez_id, min_max_normalize, to_symbol"
      ],
      "metadata": {
        "id": "u1-LrbQBghxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interaction_methods_path = '' # should have the columns of g1, g2 (the interaction genes)\n",
        "# all data should be of type dataframe with the entrez-ids as index\n",
        "co_locations_path = ''\n",
        "co_abundance_path = ''\n",
        "h_sapiens_path = ''\n",
        "gene_expression_path = ''\n",
        "proteins_expression_path = ''\n",
        "locations_path = ''\n",
        "interaction_methods = pd.read_csv(interaction_methods_path)\n",
        "interaction_methods_dict = dict(zip(zip(interaction_methods.g1, interaction_methods.g2),\n",
        "              [t[3:] for t in interaction_methods.itertuples()]))\n",
        "location_data = pd.read_csv(locations_path)\n",
        "co_locations_data = pd.read_csv(co_locations_path)\n",
        "co_abundance = pd.read_csv(co_abundance_path)\n",
        "proteins_expressions = pd.read_csv(proteins_expression_path)\n",
        "expression_data = pd.read_csv(gene_expression_path)\n",
        "h_sapiens = pd.read_csv(h_sapiens_path)\n",
        "\n",
        "expression_len = 2\n",
        "prots_len = 3\n",
        "locations_len = 2 * location_data.shape[1] + 1\n",
        "methods_len = interaction_methods.shape[1] - 2"
      ],
      "metadata": {
        "id": "4AKuVYnMizR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_data_dir = 'human_dataset\n",
        "train_interactions = [] # devide the interaction set into three groups\n",
        "val_interactions = []\n",
        "test_interactions = []\n",
        "for interactions, dataset_name in zip([train_interactions, val_interactions, test_interactions], ['train', 'val', 'test']):\n",
        "  data, labels, edges = get_data_matrix(\n",
        "    interactions, ground_truth, gene_expression=expression_data,\n",
        "    prot_expression=pd.DataFrame(min_max_normalize(proteins_expressions)),\n",
        "    co_abundance=co_abundance,\n",
        "    locations=location_data, methods=interaction_methods_dict, co_locations=co_locations_data)\n",
        "  data['edges'] = edges\n",
        "  pd.DataFrame(data).to_csv(os.path.join(human_data_dir, f'{dataset_name}_X.csv'), index=False)\n",
        "  pd.DataFrame(labels).to_csv(os.path.join(human_data_dir, f'{dataset_name}_y.csv'), index=False)"
      ],
      "metadata": {
        "id": "J6QAVMDviGHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdww7nh3gdTu"
      },
      "outputs": [],
      "source": [
        "\n",
        "datasets = {\n",
        "    x: GatbiDataset(data=d, y_data=y, expression_size=expression_len, locations_size=locations_len,\n",
        "                  prot_size=prots_len, methods_size=methods_len) for x, d, y in\n",
        "    zip(['train', 'val', 'test'], [X_train, X_val, X_test], [y_train, y_val, y_test])}\n",
        "\n",
        "torch.manual_seed(seed=1234)\n",
        "np.random.seed(1234)\n",
        "model = GatsbiNet(expression_size=expression_len, locations_size=locations_len, prot_size=prots_len, graph_matrix=datasets['train'].graph_matrix,\n",
        "                  second_input_size=datasets['train'].interaction.shape[-1],\n",
        "                  hidden_size=64, p=0.3)\n",
        "model.train_all(datasets, epochs=750, learning_rate=1e-3)\n"
      ]
    }
  ]
}
